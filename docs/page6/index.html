<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Moments of Clarity &middot; 
    
  </title>

  
  <link rel="canonical" href="https://anish.lakkapragada.com/page6/">
  

  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/poole.css">
  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/syntax.css">
  <link rel="stylesheet" href="https://anish.lakkapragada.com/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://anish.lakkapragada.com/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://anish.lakkapragada.com/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://anish.lakkapragada.com/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'G-MY94EDEHE6', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="theme-base-custom">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Writings to describe the thoughts swirling in my head when they slow down and reformulate.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item active" href="https://anish.lakkapragada.com/">home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/about/">about</a>
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birdart/">art</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birds/">birds</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/categories/">categories</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/contact/">leave a note</a>
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/ideas/">ideas</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/notes/">theoretical</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
  </nav>

  <nav class="sidebar-nav">
      <details class="sidebar-nav-item"> 
        <summary> All Posts </summary>  
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/songs/thinking/2023/11/08/worthalisten/">Worth A Listen?</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/10/06/fatd/">A Review: For All The Dogs</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/10/06/songs/">Lifetimes in 3 Minutes: Music</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/09/08/bruh/">Our World, In Distributions</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/songs/2023/08/18/trophies/">Trophies by Drake</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/08/15/perception/">Series of Philosophies</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/songs/2023/07/23/club-paradise/">Club Paradise by Drake</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/07/21/hardquestions/">A Hard Question (Partially) Answered, Badly</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/05/09/tests/">On The Value of Tests</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/05/04/what-could-go-wrong/">A Growing List of Lucky Things</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/04/28/reminded/">How did We Get Here?</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/03/24/why-I-enjoy-school/">Why I Enjoy School</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birds/2023/03/14/birds-notes/">Bird Notes after a Year</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2023/02/22/naps/">Naps</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/02/01/update/">Series of Observations</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2023/01/11/hedonistic-happy/">Hedonistic Treadmill: What It Means to Be Happy With What You Have</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2023/01/01/double-backprop/">Double Backpropagation: A Forgotten Algorithm</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2022/12/22/history/">PSA Reminder: History Actually Happened</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2022/12/10/beards/">My Experience Growing a Beard, And Then Shaving It</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2022/11/24/crawdads-tkam/">Comparison of Where The Crawdads Sing and To Kill a Mockingbird</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2022/11/09/word-association/">Food for Thought: Word Predictability</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/11/04/latex/">Why Does Latex look so good?</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/random/2022/10/24/act/">Falling Forward, Not Back: Drake</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/10/11/gradient-descent-euler/">Gradient Descent Revisited As Euler's Method</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/birds/2022/10/04/birdslist/">Photograph Birds List</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/09/26/mltaylorseries-copy/">Hidden Taylor Series in Theoretical Machine Learning</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/thinking/2022/09/25/paretoprinciple/">An explanation of Pareto's Principle</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/coding/2022/09/21/sveltevreact/">Yet Another Comparison of Svelte & React</a>
          
            <a class="sidebar-nav-item" href="https://anish.lakkapragada.com/theoretical/2022/09/20/closed-form-logreg/">Attempts at Closed-Form Logistic Regression</a>
          
      </details>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2023. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="home">Moments of Clarity</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="message" style="font-size: 0.9em">
  Hey y'all! Same blog, different (<a href="https://github.com/poole/lanyon"
    >Lanyon
  </a>
  themed) look. <br />
  If you are here for my theoretical articles or notes, they are
  <a href="/notes"> here</a>. <br />
  The sidebar also contains all posts separated by categories
  <a href="/categories"> here. </a> <br />
  Hope you enjoy!
</div>

<!-- Mailchimp Emailing List Form -->

<div id="mc_embed_shell">
  <link
    href="//cdn-images.mailchimp.com/embedcode/classic-061523.css"
    rel="stylesheet"
    type="text/css"
  />
  <style type="text/css">
    #mc_embed_signup{background:#fff; false;clear:left; font:14px Helvetica,Arial,sans-serif; width: 600px;}
    /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
  </style>
  <div id="mc_embed_signup">
    <form
      action="https://lakkapragada.us21.list-manage.com/subscribe/post?u=14f2c1767aebf590c27ef3978&amp;id=d9dd760afb&amp;f_id=00e7dee6f0"
      method="post"
      id="mc-embedded-subscribe-form"
      name="mc-embedded-subscribe-form"
      class="validate"
      target="_blank"
    >
      <div id="mc_embed_signup_scroll">
        <h2>Subscribe to Moments of Clarity Blog</h2>
        <div class="indicates-required">
          <span class="asterisk">*</span> indicates required
        </div>
        <div class="mc-field-group">
          <label for="mce-EMAIL"
            >Email Address <span class="asterisk">*</span></label
          ><input
            type="email"
            name="EMAIL"
            class="required email"
            id="mce-EMAIL"
            required=""
            value=""
          /><span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
        </div>
        <div id="mce-responses" class="clear foot">
          <div
            class="response"
            id="mce-error-response"
            style="display: none"
          ></div>
          <div
            class="response"
            id="mce-success-response"
            style="display: none"
          ></div>
        </div>
        <div aria-hidden="true" style="position: absolute; left: -5000px">
          /* real people should not fill this in and expect good things - do not
          remove this or risk form bot signups */
          <input
            type="text"
            name="b_14f2c1767aebf590c27ef3978_d9dd760afb"
            tabindex="-1"
            value=""
          />
        </div>
        <div class="optionalParent">
          <div class="clear foot">
            <input
              type="submit"
              name="subscribe"
              id="mc-embedded-subscribe"
              class="button"
              value="Subscribe"
            />
            <p style="margin: 0px auto">
              <a
                href="http://eepurl.com/iDLimE"
                title="Mailchimp - email marketing made easy and fun"
                ><span
                  style="
                    display: inline-block;
                    background-color: transparent;
                    border-radius: 4px;
                  "
                  ><img
                    class="refferal_badge"
                    src="https://digitalasset.intuit.com/render/content/dam/intuit/mc-fe/en_us/images/intuit-mc-rewards-text-dark.svg"
                    alt="Intuit Mailchimp"
                    style="
                      width: 220px;
                      height: 40px;
                      display: flex;
                      padding: 2px 0px;
                      justify-content: center;
                      align-items: center;
                    " /></span
              ></a>
            </p>
          </div>
        </div>
      </div>
    </form>
  </div>
  <script
    type="text/javascript"
    src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"
  ></script>
  <script type="text/javascript">
    (function ($) {
      window.fnames = new Array();
      window.ftypes = new Array();
      fnames[0] = "EMAIL";
      ftypes[0] = "email";
      fnames[1] = "FNAME";
      ftypes[1] = "text";
      fnames[2] = "LNAME";
      ftypes[2] = "text";
      fnames[3] = "ADDRESS";
      ftypes[3] = "address";
      fnames[4] = "PHONE";
      ftypes[4] = "phone";
      fnames[5] = "BIRTHDAY";
      ftypes[5] = "birthday";
    })(jQuery);
    var $mcj = jQuery.noConflict(true);
  </script>
</div>

<div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/theoretical/2022/09/26/mltaylorseries-copy/"> Hidden Taylor Series in Theoretical Machine Learning </a>
    </h1>

    <span class="post-date">26 Sep 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Taylor Series, Calculus, Gradient Descent, Polynomial Regression, Theoretical ML</strong></p>

<p>This article hopes to provide an alternate look at some of the most foundational algorithms in machine learning, namely Gradient Descent and Polynomial Regression, from an angle of Taylor Series.
Taylor Series are an extremely nice approximation method from calculus and are actually quite common in machine learning.</p>

<blockquote>
  <p>Machine Learning is about creating functions to model (i.e. approximate) functions in data - Taylor series is about approximating functions as well; it should come as no suprise that they are related in many cases.</p>
</blockquote>

<h2 id="taylor-series-primer">Taylor Series Primer</h2>

<p>So, what are taylor series? I won’t go into the proof here, because I don’t remember it and I don’t think its required, but the main detail is that Taylor Series are a way to approximate any function \(f(x)\) at a given point \(c\) using an infinite sum of polynomials. It’s formula is shown below:</p>

\[f(x)=\sum_{n=0}^\infty f^{(n)}(c)\frac{(x-c)^n}{n!} = f(c) + f^{(1)}(c)(x-c) + \dfrac{f^{(2)}(c)(x-c)^2}{2} + \ldots\]

<p>Where \(f^{(n)}(c)\) represents the \(n\)-th derivative (or just \(f(c)\) if \(n\) is 0), at a point of \(x=c\). Above, we only explicitly show the summation up to the second degree term - if we were to remove the other terms \(\ldots\), we would be left with the <em>second-degree approximation</em> of \(f(x)\). Using finite approximations of a taylor series will become important later, as infinite summations are not always possible in a computer.</p>

<p>On with the examples!</p>

<h2 id="taylor-series-in-gradient-descent">Taylor Series in Gradient Descent</h2>

<p><em>Before reading about the usage of Taylor Series in gradient descent, keep in mind that many different calculus concepts (not just taylor series!) play into gradient descent.</em></p>

<p>Gradient Descent is an iterative algorithm to generally optimize some function overtime based on its gradient (vector of partial derivatives.) The gradient yields a vector that tells the fastest way to ascend a curve - with gradient <em>descent</em> you try to go <em>down</em> the curve and thus constantly move in the negative of this gradient (moving with the positive is called gradient ascent). Gradient Descent is shown below.</p>

\[\theta_{t} = \theta_{t - 1} - \alpha * \nabla_{\theta_{t - 1}} J\]

<p>For more clarification, \(\theta_{t}\) are the parameters of the model (gradient descent is model agnostic, so this could range from linear regression to GPT-3) at a timestep \(t\) and \(\nabla_{\theta_{t - 1}} J\) represents the gradient of the objective function \(J\) that we are trying to minimize. It’s not shown, but \(J\) takes in the parameters \(x, y\) and \(\theta\).  As stated, we move in the direction of the negative of the gradient. \(\alpha\) is the learning rate and is applied to scale the gradient and prevent too high movements.</p>

<p>Gradient Descent can be re-thought of as finding some value \(\Delta \theta\) to adjust \(\theta_{t - 1}\) at each iteration \(t\) such that \(J(\theta_{t - 1} + \Delta \theta)\) is less than \(J(\theta_{t - 1})\). This is where taylor series come in - they help us find this required change. The taylor series to approximate the new value of the objective function to the first-degree is shown below.</p>

<blockquote>
  <p>Disclaimer: My linear algebra and multivariable calculus skills are kinda DNE, so please don’t try to inspect every term and make sure that the shapes all match up (probably missing a transpose here and there). Instead try to built intuition of the general approach.</p>
</blockquote>

\[J(\theta_{t - 1} + \Delta \theta) \approx J(\theta_{t - 1}) + \nabla_{\theta_{t - 1}} J * \Delta \theta\]

<p>This is a bit confusing, so let’s clarify, in this case the \(x\) in the approximation of \(f(x)\) is actually \(\theta_{t - 1} + \Delta \theta\) and thus the taylor series is centered at a point \(c\) of \(\theta_{t - 1}\). This means that the first-degree term \(x - c\) actually equals just \(\Delta \theta\).</p>

<p>This is where it gets clever. Given that we set \(J(\theta_{t - 1})\) or \(J(\theta_{t - 1} + \Delta \theta)\) to be less than \(J(\theta_{t - 1})\), to make both sides of the approximation equal, we basically need to find a way to balance the right and left side by reducing \(J(\theta_{t - 1})\) with negatives from the first-degree expression (\(\nabla_{\theta_{t - 1}} J * \Delta\theta\)) to it. We could just set \(\Delta \theta\) to \(-1\), but because \(\nabla_{\theta_{t - 1}} J\) is a matrix - there is no guarantee that all of the numbers inside of it will necessarily be the same sign. Thus we need to find some value of \(\Delta \theta\) that will make \(\nabla_{\theta_{t - 1}} J\) ALL negative.</p>

<p>This is actually much easier than expected. Just set \(\Delta \theta\) to the <em>negative</em> of \(\nabla_{\theta_{t - 1}} J\) and we guarantee all elements are negative (as a gradient squared yields all positive values.) This leads to the approximation having any chance of balancing out. As we add more degrees to the taylor polynomial, this approximation would keep on getting better.</p>

<p>This means that in our (minimization) algorithm of Gradient Descent, with \(\alpha\) for stability, we have:</p>

\[\theta_{t} = \theta_{t - 1} + \Delta \theta_{t - 1} = \theta_{t - 1} - \alpha * \nabla_{\theta_{t - 1}} J\]

<p>That’s gradient descent!</p>

<p>A lot of this information was taken from <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos597G/lecnotes/lecture3.pdf">here</a> and summarized here. We can expand this method to actually include second and third derivatives as well - but those aren’t used as much due to requiring more computational power (sorry for the explanation that literally <em>everyone</em> gives.)</p>

<h2 id="taylor-series-in-polynomial-regression">Taylor Series in Polynomial Regression</h2>

<p>Weeee that was a lot of work for gradient descent! Luckily, applications of taylor series in polynomial regression is much more straightforward.</p>

<p>Polynomial regression is basically a type of linear regression where a single input (let’s stick to one-dimensional input for simplicity) is raised to higher powers and then the appropriate coefficients are found. The prediction function for a two-degree polynomial regression is shown below.</p>

\[\hat{y} = h(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}\]

<p>As shown above, \(\hat{y}\) are the predictions and \(\beta_{n}\) are the coefficients for \(x\) raised to the \(n\)-th power. Already looks like a taylor series (or maclaurin series as \(c = 0\)) right?</p>

<p>In fact, it is! It’s that simple. Let’s see if this actually works in practice.</p>

<h3 id="empirical-experiment-with-exponentials">Empirical Experiment with Exponentials</h3>

<p>Alliteration, huh? Okay, so the famous taylor series of the function \(e^{x}\) is shown below.</p>

\[e^{x} = \sum_{n = 0}^{\infty} \dfrac{x^{n}}{n!} = 1 + x + \dfrac{x^{2}}{2} + \ldots\]

<p>Will Polynomial Regression (to a degree of 2) actually learn this specific taylor series? To reiterate, if I train a Linear Regression model with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">scikit-learn</a>’s in Python, that takes in \(x\) and \(x^{2}\) as input - will it learn \({1, 1, 0.5}\) as \(\beta_{0}, \beta_{1}, \beta\_{2}\) respectively?</p>

<p>Will this taylor series be learned, from an algorithm derived from taylor series 🤯 ?</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="s">"""
we'll generate from a normal distribution as the
second-degree polynomial approximates best in this range.
"""</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="p">))</span> <span class="c1"># 1000 standard normal samples
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">e</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># labels
</span></code></pre></div></div>

<p>We can now construct the \(X^{2}\) data. We’ll merge into <code class="language-plaintext highlighter-rouge">X_poly</code> which will contain two columns - the first one for \(X\) and the second one for \(X^{2}\).</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">X_2</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s apply Linear Regression.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lin_reg</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span> <span class="c1"># lin reg algorithm
</span><span class="n">lin_reg</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s inspect the coefficients \(\beta_{1}, \beta_{2}\).</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">lin_reg</span><span class="p">.</span><span class="n">coef_</span>
<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.24106858</span><span class="p">,</span>  <span class="mf">2.25427569</span><span class="p">])</span>
</code></pre></div></div>

<p>And the bias \(\beta_{0}\).</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">lin_reg</span><span class="p">.</span><span class="n">intercept_</span>
<span class="mf">1.5053263303981406</span>
</code></pre></div></div>

<p>Looks like it won’t.</p>

<p>My hypothesis is that if you use a higher-degree approximation (and of course have much more data), it will be more likely the coefficients will slowly fall in line with the <em>true</em> taylor polynomial. In our case, given that it only had a sample of \(e^{x}\) and not the whole infinite set of values, it is to be expected that not the precise values were found as these values actually could have minimized the mean-squared error more for this specific training set \(X\).</p>

<p>Also, if Polynomial Regression is a taylor series polynomial then linear regression is a first-degree taylor series?</p>

<h2 id="review">Review</h2>

<p>Congrats on making it here!</p>

<p>Taylor Series are a great way to approximate any function into polynomials. They have a lot of hidden applications in machine learning mathematics. We went over their role in helping determine how to move iteratively in Gradient Descent and it’s very clear application to Polynomial Regression.</p>

<p>Please let me know what more theoretical machine learning applications you find! Thanks for reading.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/thinking/2022/09/25/paretoprinciple/"> An explanation of Pareto's Principle </a>
    </h1>

    <span class="post-date">25 Sep 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Pareto’s Principle, 80/20 rule, Distribution of Outcomes, Applications</strong></p>

<p>A while ago (more like 4 years ago) I discovered Pareto’s principle - also known as the 80/20 rule - which states that’s 80% of outputs result from 20% of the inputs. Less abstractly, this means that 20% of the input to anything, such as time or effort, will lead to 80% of the actual outputs. For example, only 20% of the features on Microsoft Word will actually be used by 80% of the users whereas the remaining 80% of the features will be used by 20% of the users.</p>

<p>Pareto’s distribution is stunningly applicable. 80% of the crimes are commited by 20% of registered criminals, and 80% of a business’s wealth comes from only 20% of the clients. 20% of the apps on your phone likely are used 80% of the time. This distribution does not need to be exactly 80/20; in some cases it can even go down to 95/5 or even 99/1 (e.g. distribution of wealth across population).</p>

<p>So, how can we use Pareto’s principle in our daily lives? Internalizing the exponential nature in our lives is probably the best way to implement it. If we want a 90 on a test, we’ll only need to put in about half of the effort and time studying compared to getting a 97+. The same goes for standardizing testing; scores are gradually harder to get at the top. Keeping in mind the 80/20 rule can help us decide when the extra mile is excessive or essential.</p>

<p>So now we’ve gone over the how and what - only the <em>why</em> remains. Any ideas on why our world and outcomes are distributed this way? My guess is that the underlying reason of the 80/20 rule is disparity and inequity - one group having much more extreme outliers that overpower all others. The world is moving at an <a href="https://www.su.org/blog/thriving-in-an-exponential-world-and-making-a-difference-doing-so">exponential rate</a>; the most successful stocks (which are extremes in themselves) almost always follow exponential curves as compared to linear ascents. Moore’s law literally forecasts that the number of transistors on a microchip double every 2 years; it’s no secret that <a href="https://en.wikipedia.org/wiki/Accelerating_change">we also think the world is constantly growing faster</a> than it ever has before. In short, Pareto’s principle seems to be very common when disparity or a gap grows uncontrollably. In our world, that’s not too uncommon.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/coding/2022/09/21/sveltevreact/"> Yet Another Comparison of Svelte & React </a>
    </h1>

    <span class="post-date">21 Sep 2022</span>

    <p><strong>Keywords: Reactive Frameworks, Svelte, React, JavaScript</strong></p>

<p>About a year ago, I was introduced to the ever-evolving frontend world where JavaScript frameworks keep on <a href="https://dayssincelastjavascriptframework.com/">getting released every single day</a> and would like to share my comparison on two frameworks I’ve worked with the most. In no way am I an expert on either of these frameworks, but I think it’s a healthy exercise for me to understand them better by drawing a comparison between them.</p>

<p>React is a framework brought to us by Facebook and is widely considered the most popular JavaScript frontend framework there is. Svelte meanwhile is a relatively new framework introduced by Rich Harris, a journalist for the New York Times, focused on developing small-scale, light web applications. As far as Stack Overflow surveys go, there is an interesting split between them - while React is the most <em>wanted</em> framework, Svelte is the most <em>loved</em> framework among current developers. Based on my experience thus far, I kind of can see why that is. More on that later. For now, here are the major differences that I felt.</p>

<h2 id="1-usestate-in-react-sucks">1. useState() in React Sucks</h2>

<p>This is probably the biggest thing I can think about between React and Svelte. In Svelte, whenever you have a reactive variable you want to change (which also is used in rendering for-loops or conditionals), you just change it. Just <code class="language-plaintext highlighter-rouge">foo = "bar"</code> and you are done!</p>

<p>In React, though, a clunky state management solution is provided, where you have to first initialize the default value with <code class="language-plaintext highlighter-rouge">useState(default)</code> and then use the provided functions (e.g. <code class="language-plaintext highlighter-rouge">foo</code> and <code class="language-plaintext highlighter-rouge">setFoo</code>) to update your variables instead of assignment (as aforementioned for Svelte.)</p>

<p><img src="https://preview.redd.it/twvap8pq9fg91.png?width=680&amp;format=png&amp;auto=webp&amp;s=9bd1e81563e26210644561038b221d25b481bc23" /></p>

<p>Because I started out with Svelte and then React, I forget so many times (often for 2 hours at a time) that I can’t just use assignment and instead need to go with <code class="language-plaintext highlighter-rouge">useState</code>. Considering React is older than Svelte, it is somewhat expected that Svelte would have the edge on this one.</p>

<h2 id="2-react-styling">2. React Styling</h2>

<p>In React, you can either attach another stylesheet or you can use inline styles (in JSON) to the individual components themselves. This is likely just personal preference, but I find it annoying to have to link each component to another css file (~2x more files that way), or to use inline styles.</p>

<p>Compared to this, Svelte offers either using inline styling or separate styles in the same file as your HTML the same way as vanilla HTML. This is one of the examples where Svelte demonstrates its ridiculously low learning curve.</p>

<h2 id="3-performance">3. Performance</h2>

<p>React is said to treat your model as a blackbox and calculate the difference between what is currently on the page and what should be on the page in a theoretical DOM known as the virtual DOM. Svelte, on the other hand, takes the approach of not using the virtual DOM and instead compiling components at build in a way where the code will take care of whatever rendering changes need to take place. This has been noted to make Svelte have the edge in performance; the Svelte website goes as far as saying the idea that the vDOM makes applications faster is a <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">“suprising resilient meme”</a>.</p>

<p>The main idea stated by Svelte is that even if the DOM is slow, adding another vDOM will only make things slower as the native DOM will eventually have to be changed. Furthermore, they point that the <code class="language-plaintext highlighter-rouge">useState</code> function in React can in some cases lead to parts of applications rerendering even when not needed.</p>

<p>For fairness, I’ve only really argued for Svelte thus far (and based pretty closely on their own article.) However, other sources also confirm this and the fact that Svelte is <a href="https://massivepixel.io/blog/svelte-vs-react/">nearly 26x more lightweight compared to React</a>.</p>

<h2 id="4-popularity">4. Popularity</h2>

<p>It would be foolish to discard popularity in this discussion. React, hands down, has more popularity. Svelte has 62K stars, React has almost 200K and has been used in plenty of websites.</p>

<p>Thus, it follows that when it comes to UI libraries and other community open-source tools React likely will have much better support.</p>

<h2 id="final-remarks">Final Remarks</h2>

<p>Both Svelte and React are decent frameworks with intended uses. Given the popularity difference,Svelte makes sense to be primarily for much smaller stuff, whereas React is for building larger websites.</p>

<p>Back to what I was saying about the most <em>wanted</em> vs. most <em>loved</em>. Thus far, I still prefer Svelte to React. It’s extremely similar to raw HTML/CSS/JS and takes basically no effort to learn (very easy learning curve.) In fact, I think it’s easier to learn Svelte first than it is to learn HTML and JS first. However, despite me loving Svelte, I felt that I needed to learn React to collaborate with others on projects, such as the current project to build a coding competition website and grading server for our school’s computer science club with 3-4 other devs.</p>

<p>While React isn’t as good as Svelte for me, it isn’t terrible and <em>it’s allowing bigger collaborations with more people. After all, that probably matters more.</em></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://anish.lakkapragada.com/theoretical/2022/09/20/closed-form-logreg/"> Attempts at Closed-Form Logistic Regression </a>
    </h1>

    <span class="post-date">20 Sep 2022</span>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Keywords: Logistic Regression, Regression, Normal Equation, Logistic Function, Optimization</strong></p>

<p>Let’s work our way up from the start. Linear Regression is a ubiquitous algorithm in machine learning today, which is most commonly performed through iterative gradient descent. However, another method that I find pretty fascinating is the closed-form solution called the <em>Normal Equation</em>, where instead of iteratively trying to minimize \(L = \sum_{i=1}^{N} (mx_{i} + b - y_{i})^2\), (\({m, b}\) are the parameters), a solution of what value of \(m\) sets \(\dfrac{dL}{dm}\) to 0 is found. The bias \(b\) is then found as \(\bar{y} - m\bar{x}\), where \(\bar{z}\) is the average of a set \(z\). Thus this solution is called <em>closed-form</em> as it takes a known amount of mathematical operations to solve.</p>

<p>However, this is old news. Unfortunately, this style of optimization is only possible for simple linear regression. The main reason for this is that the derivative of a linear model is really simple, compared to something like a composite neural network, which likely will require endless chain rule. The closest thing I could find to Linear Regression that has a different shape (hence ridge regression not included) is Logistic Regression, where predictions of \(y_{i}\) are modeled as \(\sigma(mx_{i} + b)\), where \(\sigma\) is the logistic (hence the name) function. The thing to remember though is that Logistic Regression is not really a regressor, but actually a binary classifier model.</p>

<p>My question is whether we actually can try to create a closed-form solution for Logistic Regression. Two years ago, I tried doing exactly <a href="https://www.reddit.com/r/MachineLearning/comments/no5q8j/p_potential_logistic_regression_closed_form/">this</a> with such a lazy method that yieled itself a rightful 0 upvotes on r/MachineLearning. Instead of this really bad method, how about trying to edit the current linear Normal Equation for our case? A really good derivation of the Normal Equation can be found <a href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/">here</a> - Perhaps, let’s see if we can modify the objective function to make it more logistic-y and see where that takes us?</p>

<p>The objective function is defined as \(L = \dfrac{(X\theta - y)^{T}(X\theta - y)}{N}\) (let’s stick to \(\theta\) instead of \(m\) to look smarter) where \(X\) is the training feature matrix, \(\theta\) is weight vector (multi-dimensional \(m\)), and \(y\) is the labels vector. We would modify this to contain \(\sigma(X\theta)- y\), and soon it becomes clear this approach probably will not work as the resulting expression (before differentiation) will contain a lot of \(\sigma(X\theta^{T})\sigma(X\theta)\) which is too annoying for my brain to work with. Nobody, promised there would be a solution right?</p>

<p>Another method that I think would be more serious, if only slightly, would be to modify the data itself with an inverse function. Exponential Regression does exactly this by applying a log to all the labels (y-values) to turn the exponential curves more linear. The model then learns a linear model to predict the \(\ln(y*{i})\) which is then exponentiated to give the actual function. Essentially, this just uses an inverse of the \(\sigma\) function before the linear outputs. So, let’s just calculate the inverse of the logistic/sigmoid function
(\(\dfrac{1}{1 + e^{-x_{i}}}\))! This comes out to be \(-\ln(\dfrac{1}{x_{i}} - 1)\). Definitely not the worst thing in the world!</p>

<p>Let’s specify this inverse as \(\sigma'\) and take a step back. We need to apply the function \(\sigma'\) to every label to convert it to a line, train a linear model (closed-form), and then take linear productions and run them through \(\sigma\) (actual logistic function.) We probably should take a look at the domain of this inverse function, and see if our labels are valid in this range. Unfortunately the inverse of the logistic function is not defined for all real numbers and has semi-sharp asymptotes at \(x=0\) and \(x=1\). Even more unfortunate, our labels are binary integers of 0 or 1!</p>

<p>So far, we have tried a closed-form solution and then layering modifications of inputs and predictions on top of a standard linear closed-form solve. If anything, these examples demonstrate why the logistic regression has no closed-form solution. I believe it is always better to fail yourself than to take somebody else’s word that the <a href="https://youtu.be/32ZemGEYraY?t=76">transcedental equation</a> is why there is no logistic regression closed-form solution. The intuitive explanation I could come up with is that closed-form solutions don’t work as well when not trying to draw a line not to predict values (regression) but instead trying to separate areas (classification).</p>

<p>This is the end of our journey. Please let me know what errors I may have made or whatever you find. Thanks.</p>

  </div>
  
</div>

<div class="pagination">
  
  <span class="pagination-item older">Older</span>
    
  <a
    class="pagination-item newer"
    href="https://anish.lakkapragada.com/page5"
    >Newer</a
  >
   
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
